{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f667a43",
   "metadata": {},
   "source": [
    "# Taller 6 - Búsqueda por Similaridad Coseno (Mini Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a151d8e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f51cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters as nltk_reuters\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb59f8e",
   "metadata": {},
   "source": [
    "## Funciones de Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa621ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_directory(data_dir: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Lee todos los archivos .txt de un directorio.\"\"\"\n",
    "    p = Path(data_dir)\n",
    "    if not p.exists() or not p.is_dir():\n",
    "        raise FileNotFoundError(f\"Directorio no encontrado: {data_dir}\")\n",
    "\n",
    "    doc_ids, docs = [], []\n",
    "    for name in sorted(os.listdir(p)):\n",
    "        if name.lower().endswith(\".txt\"):\n",
    "            doc_ids.append(name)\n",
    "            with open(p / name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                docs.append(f.read())\n",
    "    if not docs:\n",
    "        raise RuntimeError(f\"No se encontraron .txt en {data_dir}\")\n",
    "    return doc_ids, docs\n",
    "\n",
    "\n",
    "def read_reuters_plain(path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Lee un archivo Reuters en formato plain (reut2-XXXX.plain).\"\"\"\n",
    "    raw = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    parts = re.split(r'<REUTERS ID=\"(\\d+)\">\\s*', raw)\n",
    "    ids = parts[1::2]\n",
    "    texts = [t.strip() for t in parts[2::2]]\n",
    "    doc_ids = [f\"reut-{i}\" for i in ids]\n",
    "    return doc_ids, texts\n",
    "\n",
    "\n",
    "def read_nltk_reuters() -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Lee el corpus Reuters desde NLTK.\"\"\"\n",
    "    if nltk is None or nltk_reuters is None:\n",
    "        raise RuntimeError(\"NLTK/Reuters no está disponible. Instale nltk y ejecute nltk.download('reuters', 'punkt').\")\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/reuters\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"reuters\")\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    fileids = nltk_reuters.fileids()\n",
    "    texts = [nltk_reuters.raw(fid) for fid in fileids]\n",
    "    return fileids, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9954fb",
   "metadata": {},
   "source": [
    "## Funciones de Vectorización y Búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b983ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorizer(ngram_max: int = 1) -> TfidfVectorizer:\n",
    "    \"\"\"Construye un vectorizador TF-IDF.\"\"\"\n",
    "    vec = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1, ngram_max),\n",
    "        stop_words=\"english\",\n",
    "        max_df=0.9,      \n",
    "        min_df=2,       \n",
    "        dtype=np.float32,\n",
    "        norm=\"l2\",       \n",
    "    )\n",
    "    return vec\n",
    "\n",
    "\n",
    "def sparsity_report(X) -> str:\n",
    "    \"\"\"Genera un reporte de la dispersión de la matriz TF-IDF.\"\"\"\n",
    "    nnz = X.nnz\n",
    "    total = X.shape[0] * X.shape[1]\n",
    "    density = nnz / total if total > 0 else 0.0\n",
    "    return f\"Docs: {X.shape[0]:,} | Vocab: {X.shape[1]:,} | NNZ: {nnz:,} | Densidad: {density:.6f} (~{density*100:.4f}%)\"\n",
    "\n",
    "\n",
    "def search(\n",
    "    query: str,\n",
    "    vec: TfidfVectorizer,\n",
    "    X,\n",
    "    doc_ids: List[str],\n",
    "    top_k: int = 10,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Busca documentos similares a la consulta usando similaridad coseno.\"\"\"\n",
    "    qv = vec.transform([query]) \n",
    "    sims = cosine_similarity(X, qv, dense_output=False).toarray().ravel()\n",
    "    top_idx = np.argsort(-sims)[:top_k]\n",
    "    return [(doc_ids[i], float(sims[i])) for i in top_idx if sims[i] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f32d8",
   "metadata": {},
   "source": [
    "## Configuración y Carga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b747d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo Reuters plain: ../data/reut2-1000.plain\n",
      "Documentos cargados: 1,000\n"
     ]
    }
   ],
   "source": [
    "USE_NLTK = False \n",
    "DATA_DIR = \"../data\" \n",
    "REUTERS_PLAIN = \"../data/reut2-1000.plain\" \n",
    "NGRAM_MAX = 1 \n",
    "\n",
    "if REUTERS_PLAIN and Path(REUTERS_PLAIN).exists():\n",
    "    print(f\"Leyendo archivo Reuters plain: {REUTERS_PLAIN}\")\n",
    "    doc_ids, docs = read_reuters_plain(REUTERS_PLAIN)\n",
    "elif USE_NLTK:\n",
    "    print(\"Leyendo corpus Reuters desde NLTK...\")\n",
    "    doc_ids, docs = read_nltk_reuters()\n",
    "else:\n",
    "    print(f\"Leyendo .txt desde: {DATA_DIR}\")\n",
    "    doc_ids, docs = read_txt_directory(DATA_DIR)\n",
    "\n",
    "print(f\"Documentos cargados: {len(docs):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb5c8a",
   "metadata": {},
   "source": [
    "## Construcción de la Matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdf302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TF-IDF (CSR) creada.\n",
      "Docs: 1,000 | Vocab: 5,694 | NNZ: 57,512 | Densidad: 0.010100 (~1.0100%)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = build_vectorizer(NGRAM_MAX)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Matriz TF-IDF (CSR) creada.\")\n",
    "print(sparsity_report(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f6a66",
   "metadata": {},
   "source": [
    "## Realizar Búsquedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ed3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: \"british jaguar sales\"\n",
      "\n",
      "Top 10 resultados:\n",
      " 1. reut-1218                                           score=0.337039\n",
      " 2. reut-612                                            score=0.270653\n",
      " 3. reut-957                                            score=0.267966\n",
      " 4. reut-888                                            score=0.267966\n",
      " 5. reut-1179                                           score=0.252651\n",
      " 6. reut-670                                            score=0.227302\n",
      " 7. reut-57                                             score=0.205724\n",
      " 8. reut-932                                            score=0.162981\n",
      " 9. reut-726                                            score=0.160041\n",
      "10. reut-617                                            score=0.158207\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"british jaguar sales\" \n",
    "TOP_K = 10  \n",
    "\n",
    "print(f'Consulta: \"{QUERY}\"')\n",
    "results = search(QUERY, vectorizer, X, doc_ids, top_k=TOP_K)\n",
    "\n",
    "if not results:\n",
    "    print(\"No se encontraron documentos relevantes (score > 0).\")\n",
    "else:\n",
    "    print(f\"\\nTop {len(results)} resultados:\")\n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        try:\n",
    "            cats = \", \".join(nltk_reuters.categories(doc_id)) if USE_NLTK else \"\"\n",
    "            cat_str = f\"  cats=[{cats}]\" if cats else \"\"\n",
    "        except:\n",
    "            cat_str = \"\"\n",
    "        print(f\"{rank:>2}. {doc_id:<50}  score={score:.6f}{cat_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c006d9",
   "metadata": {},
   "source": [
    "## Mostrar Snippets de los Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478cb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snippets:\n",
      " 1. reut-1218                                         \n",
      "    ... JAGUAR JAGRY FEBRUARY U.S. SALES FALL     LEONIA N.J. March 3   Jaguar PLC's Jaguar Cars Inc U.S. su ...\n",
      "\n",
      " 2. reut-612                                          \n",
      "    ... SCOTTY'S SHB SALES UP FIVE PCT IN FEBRUARY     WINTER HAVEN Fla March 2   Scotty's Inc said sales for the four we ...\n",
      "\n",
      " 3. reut-957                                          \n",
      "    ...     TOKYO March 3   Japanese investor interest in British gilt edged securities is growing rapidly due to expectations sterling will remain stable des ...\n",
      "\n",
      " 4. reut-888                                          \n",
      "    ...     TOKYO March 3   Japanese investor interest in British gilt edged securities is growing rapidly due to expectations sterling will remain stable des ...\n",
      "\n",
      " 5. reut-1179                                         \n",
      "    ... WALGREEN WAG FEBRUARY SALES RISE     DEERFIELD Ill March 3   Walgreen Co said its sales in February rose 18.8 pct over sal ...\n",
      "\n",
      " 6. reut-670                                          \n",
      "    ... NOLAND NOLD FEBRUARY SALES OFF TWO PCT     NEWPORT NEWS Va March 2   Noland Co said February sales were off 2.3 pct to 29 ...\n",
      "\n",
      " 7. reut-57                                           \n",
      "    ... n credit guarantees previously earmarked to cover sales of dry edible beans to Honduras have been switched to cover sales of white corn the U.S. Agric ...\n",
      "\n",
      " 8. reut-932                                          \n",
      "    ... ke a 15 pct stake in Air U.K. Ltd a subsidiary of British and Commonwealth Shipping Plc BCOM.L in a transaction worth around two mln stg    A KLM spok ...\n",
      "\n",
      " 9. reut-726                                          \n",
      "    ... PALL CORP PLL 2ND QTR SALES RISE 17 PCT     CHICAGO March 2   Pall Corp said sales in the second quarter rose about 17 pct ...\n",
      "\n",
      "10. reut-617                                          \n",
      "    ... PALL CORP PLL SECOND QUARTER SALES ORDERS UP     CHICAGO March 2   Pall Corp said sales for the second quarter ended January 31 w ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    try:\n",
    "        id2text = {i: t for i, t in zip(doc_ids, docs)}\n",
    "        query_terms = [w for w in QUERY.lower().split() if len(w) > 1]\n",
    "        \n",
    "        print(\"\\nSnippets:\")\n",
    "        for rank, (doc_id, score) in enumerate(results, 1):\n",
    "            text = id2text[doc_id]\n",
    "            low = text.lower()\n",
    "            pos = min((low.find(term) for term in query_terms if term in low), default=-1)\n",
    "            if pos >= 0:\n",
    "                start = max(0, pos - 50)\n",
    "                end = min(len(text), pos + 100)\n",
    "                snippet = text[start:end].replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                snippet = text[:120].replace(\"\\n\", \" \")\n",
    "            print(f\"{rank:>2}. {doc_id:50s}\")\n",
    "            print(f\"    ... {snippet} ...\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar snippets: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303707ee",
   "metadata": {},
   "source": [
    "## Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c89328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Resultados exportados a: ../results/mini_google.txt\n"
     ]
    }
   ],
   "source": [
    "output_file = \"../results/mini_google.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Consulta: {QUERY}\\n\")\n",
    "    f.write(f\"Top {TOP_K} resultados:\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        f.write(f\"{rank}. {doc_id}  (score: {score:.6f})\\n\")\n",
    "\n",
    "        try:\n",
    "            text = id2text[doc_id]\n",
    "            low = text.lower()\n",
    "            query_terms = [w for w in QUERY.lower().split() if len(w) > 1]\n",
    "            pos = min((low.find(term) for term in query_terms if term in low), default=-1)\n",
    "            if pos >= 0:\n",
    "                start = max(0, pos - 50)\n",
    "                end = min(len(text), pos + 150)\n",
    "                snippet = text[start:end].replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                snippet = text[:150].replace(\"\\n\", \" \")\n",
    "            f.write(f\"   ... {snippet} ...\\n\")\n",
    "        except:\n",
    "            pass\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✓ Resultados exportados a: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7352e8a",
   "metadata": {},
   "source": [
    "## Prueba con Diferentes Consultas\n",
    "\n",
    "Puedes probar con diferentes consultas modificando la variable `QUERY` y volviendo a ejecutar las celdas de búsqueda.\n",
    "\n",
    "### Ejemplos de consultas:\n",
    "- `\"british jaguar sales\"`\n",
    "- `\"oil prices market\"`\n",
    "- `\"computer technology software\"`\n",
    "- `\"bank financial crisis\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56214d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta: \"oil prices market\"\n",
      "================================================================================\n",
      " 1. reut-248                                            score=0.439528\n",
      " 2. reut-127                                            score=0.408124\n",
      " 3. reut-352                                            score=0.403497\n",
      " 4. reut-144                                            score=0.346958\n",
      " 5. reut-543                                            score=0.296122\n",
      " 6. reut-489                                            score=0.286141\n",
      " 7. reut-502                                            score=0.281361\n",
      " 8. reut-349                                            score=0.267038\n",
      " 9. reut-242                                            score=0.249162\n",
      "10. reut-668                                            score=0.234668\n",
      "\n",
      "Snippets:\n",
      "1. ...  TO OPEC PACT     BAHRAIN March 1   Saudi Arabian Oil Minister Hisham Nazer reiterated the kingdom's commitment to last December's OPEC accord to boos ...\n",
      "2. ... DIAMOND SHAMROCK  DIA  CUTS CRUDE PRICES     NEW YORK FEB 26   Diamond Shamrock Corp said that effective today it had cut its contract ...\n",
      "3. ... O OPEC ACCORD     BAHRAIN March 2   Saudi Arabian Oil Minister Hisham Nazer reiterated the kingdom's commitment to last December's OPEC accord to boos ...\n",
      "4. ... OPEC MAY HAVE TO MEET TO FIRM PRICES   ANALYSTS     BY TED D'AFFLISIO Reuters     NEW YORK Feb 26   OPEC may be forced to meet bef ...\n",
      "5. ... UNOCAL UCL UNIT CUTS CRUDE OIL POSTED PRICES     LOS ANGELES March 2   Unocal Corp's Union Oil Co said it lowered its posted pr ...\n"
     ]
    }
   ],
   "source": [
    "def quick_search(query_text: str, top_k: int = 10):\n",
    "    \"\"\"Realiza una búsqueda rápida y muestra los resultados.\"\"\"\n",
    "    print(f'Consulta: \"{query_text}\"')\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = search(query_text, vectorizer, X, doc_ids, top_k=top_k)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No se encontraron documentos relevantes.\")\n",
    "        return\n",
    "    \n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        print(f\"{rank:>2}. {doc_id:<50}  score={score:.6f}\")\n",
    "    \n",
    "    print(\"\\nSnippets:\")\n",
    "    id2text = {i: t for i, t in zip(doc_ids, docs)}\n",
    "    query_terms = [w for w in query_text.lower().split() if len(w) > 1]\n",
    "    \n",
    "    for rank, (doc_id, score) in enumerate(results[:5], 1):  \n",
    "        text = id2text[doc_id]\n",
    "        low = text.lower()\n",
    "        pos = min((low.find(term) for term in query_terms if term in low), default=-1)\n",
    "        if pos >= 0:\n",
    "            start = max(0, pos - 50)\n",
    "            end = min(len(text), pos + 100)\n",
    "            snippet = text[start:end].replace(\"\\n\", \" \")\n",
    "        else:\n",
    "            snippet = text[:120].replace(\"\\n\", \" \")\n",
    "        print(f\"{rank}. ... {snippet} ...\")\n",
    "\n",
    "quick_search(\"oil prices market\", top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
